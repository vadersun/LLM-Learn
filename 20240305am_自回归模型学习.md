# 自回归模型

## 简单的生成式模型：频率分布直方图

这个过程是关于离散概率分布的采样。在这个过程中，首先需要了解概率的可能值有k个，这意味着有k个不同的事件，每个事件都有一个特定的概率。

**模型训练**：通过统计数据集中每个类别出现的频率来估计概率。这是一种经验概率，它反映了在观察到的数据中，每个类别出现的相对频率。

**推断**：直接查看每个类别的概率值。这些概率值可以是从模型训练阶段得到的，也可以是从其他来源得到的。

**采样**：这是一个从已知的概率分布中生成样本的过程。这个过程包括以下步骤：

+ **计算累计密度分布函数（CDF）**：CDF是一个函数，它给出了随机变量小于或等于某个值的概率。对于离散概率分布，CDF可以通过将每个类别的概率累加起来得到。

+ **从均匀分布中采样**：生成随机数，这些随机数是在0和1之间均匀分布的。

+ **返回使得CDF大于或等于随机数的最小类别**：这个步骤是找到满足CDF大于或等于随机数的最小类别。这个类别就是的采样结果。

## 基于似然函数的生成式模型：拟合分布、最大似然估计、随机梯度下降法

这个过程是关于如何使用数据来拟合（或学习）一个概率分布的。这在许多领域，如机器学习和统计学中，都是一个常见的任务。以下是对这个过程的理解：

**数据集**：这是用来拟合分布的数据。这些数据是从真实的概率分布中采样得到的。目标是找到一个分布，它能尽可能好地描述这些数据。

**参数化的分布**：这是想要拟合的分布的类型。例如，可能想要拟合一个正态分布，或者一个泊松分布。这个分布通常有一些参数，需要通过数据来估计这些参数。

**损失函数和优化**：损失函数是一个度量分布和数据之间差距的函数。目标是找到一组参数，它能使得损失函数最小。这通常通过优化算法来实现，例如梯度下降。

需要注意的是，训练结束得到的只是一个经验分布，并不是真实的分布。这是因为只有有限的数据，而真实的分布可能是无法完全通过有限的数据来描述的。因此，我们希望模型具有一定的生成能力，也就是说，它能够生成看起来像是从真实分布中采样得到的数据。

## Autoregressive models

在神经网络中，可以使用自回归模型来建模条件概率分布。这是因为，根据概率的链式法则，可以将一个多元概率分布分解为一系列条件概率分布的乘积。例如，对于一个三维随机变量(X, Y, Z)，有：

~~~ cpp
P(X, Y, Z) = P(X) * P(Y|X) * P(Z|X, Y)
~~~

在这个公式中，P(X)是X的边缘概率分布，P(Y|X)是给定X的条件下Y的条件概率分布，P(Z|X, Y)是给定X和Y的条件下Z的条件概率分布。

在自回归模型中，目标是预测一个序列的下一个值，基于该序列的过去值。例如，如果有一个时间序列数据，我们可能想要预测明天的值，基于过去几天的值。

在神经网络中，可以使用多层感知机（MLP）来实现这个目标。MLP是一种前馈神经网络，它由一个输入层、一个或多个隐藏层和一个输出层组成。每一层都由多个神经元组成，每个神经元都与下一层的所有神经元相连。在自回归模型的情况下，可以将过去的观察值作为输入，将下一个观察值的条件概率分布作为输出。这意味着，神经网络将学习一个函数，该函数将过去的观察值映射到下一个观察值的概率分布。

假设我们有一个时间序列数据，我们想要预测明天的值。我们可以将过去三天的值作为输入，训练我们的神经网络来预测明天的值的概率分布。这个概率分布可能是一个高斯分布，其均值和方差由神经网络的输出决定。这就是如何使用神经网络，特别是MLP，来建模自回归模型的条件概率分布的。

### 权重共享

权值共享是一种在神经网络中常见的策略，它可以减少模型的参数数量，从而减少过拟合的风险，提高模型的泛化能力。权值共享的基本思想是让网络的不同部分共享相同的参数。

| 模型名 | 共享策略 |
|-------|-------|
|递归神经网络（RNN）|在RNN中，所有的时间步都使用相同的权重。这意味着，无论处理的序列有多长，模型的参数数量都保持不变。这是因为，RNN是设计来处理序列数据的，它的目标是捕捉序列中的时间依赖性。通过在所有时间步上共享权重，RNN可以更好地捕捉这种依赖性。|
|卷积神经网络（CNN）|在CNN中，卷积层的权重是共享的。这意味着，无论输入图像有多大，卷积层的参数数量都保持不变。这是因为，CNN是设计来处理图像数据的，它的目标是捕捉图像中的局部结构。通过在所有位置上共享权重，CNN可以更好地捕捉这种结构。|
|掩膜（Masking）|在某些情况下，我们可能希望网络的某些部分共享参数，而其他部分则不共享。在这种情况下，可以使用掩膜来选择性地应用权值共享。掩膜是一个与网络权重相同大小的矩阵，其中的每个元素都是0或1。当掩膜中的元素为1时，对应的权重被共享；当掩膜中的元素为0时，对应的权重则不被共享。|

## 自注意力机制（Self-Attention）

自注意力是一种在序列数据处理中非常重要的机制，它能够帮助模型捕捉序列中的长距离依赖关系。在传统的RNN模型中，由于序列数据的前后依赖关系，模型很难捕捉到序列中距离较远的信息。而自注意力机制通过计算序列中每个元素与其他所有元素的关联度，使得模型能够直接获取到任意距离的信息。

自注意力的计算过程可以分为以下几步：

+ 对于输入序列的每个位置，使用神经网络计算出三个向量：Query（查询），Key（键），Value（值）。

+ 计算Query和Key的内积，得到的结果可以看作是序列中每个元素与其他元素的关联度。

+ 对关联度进行softmax操作，使得所有的关联度之和为1，这样就得到了每个元素对应的权重。

+ 使用这些权重对Value进行加权求和，得到最终的输出。

这个过程可以看作是模型自动学习到了序列中哪些位置的信息是重要的，哪些位置的信息是不重要的，从而能够更好地处理序列数据。

当同一层同一位置有多个Query，Key，Value时，称之为多头自注意力（Multi-Head Attention）。多头自注意力可以让模型同时关注序列中的多个不同位置，从而获取更丰富的信息。

## References

本文是结合以下内容的进一步解读，感谢原作者细致的讲解：
https://zhuanlan.zhihu.com/p/385042370
